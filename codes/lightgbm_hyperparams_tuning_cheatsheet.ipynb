{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://phdinds-aim.github.io/time_series_handbook/08_WinningestMethods/lightgbm_m5_tuning.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Rules:\n",
    "\n",
    "1. Number of Estimators/Trees\n",
    "2. Depth of Tree (i) -> Num of Leaves 2**i (total leaves in a tree)\n",
    "3. Learning Rate\n",
    "4. Boosting type\n",
    "\n",
    "\n",
    "‚úî For large datasets, tune `max_depth` to avoid deep trees that slow down training.\n",
    "\n",
    "‚úî For small datasets, tune `min_data_in_leaf` to prevent overfitting.\n",
    "\n",
    "‚úî Start with `min_data_in_leaf`, then adjust `max_depth` only if needed.\n",
    "\n",
    "üöÄ Best practice: Set `min_data_in_leaf` first, then tune `max_depth` as needed!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    A[Start] --> B[Set Objective & Metric]\n",
    "    B --> C[Tune Model Complexity]\n",
    "    C --> D[Regularization to Prevent Overfitting]\n",
    "    D --> E[Optimize Speed]\n",
    "    E --> F[Enable GPU (Optional)]\n",
    "    F --> G[Tuning Strategies]\n",
    "    G --> H1[Grid Search (Slow but Exhaustive)]\n",
    "    G --> H2[Bayesian Optimization (Fast & Efficient)]\n",
    "    H1 --> I[Evaluate Model Performance]\n",
    "    H2 --> I\n",
    "    I --> J[Select Best Hyperparameters]\n",
    "    J --> K[Train Final Model]\n",
    "    K --> L[Deploy Model]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **üìå Detailed Table for LightGBM Hyperparameter Tuning**\n",
    "\n",
    "| **Category**          | **Hyperparameter**        | **Effect** | **Recommended Values** |\n",
    "|----------------------|------------------------|------------|----------------|\n",
    "| **1. Core Settings**  | `objective` | Defines the task type (Regression, Classification, etc.) | `\"regression\"`, `\"binary\"`, `\"multiclass\"` |\n",
    "|                      | `metric` | Evaluation metric for the model | `\"rmse\"`, `\"mae\"`, `\"logloss\"`, `\"auc\"` |\n",
    "|                      | `learning_rate` | Controls step size per iteration | `0.01 - 0.1` (Lower = better generalization) |\n",
    "|                      | `n_estimators` | Number of boosting rounds (trees) | `500 - 5000` (Use **early stopping**) |\n",
    "| **2. Model Complexity** | `num_leaves` | Controls tree complexity (More = Higher Accuracy, Risk of Overfitting) | `20 - 150` (Start with `31`) |\n",
    "|                      | `max_depth` | Limits the depth of trees | `4 - 12` |\n",
    "|                      | `min_data_in_leaf` | Minimum samples per leaf node | `10 - 50` |\n",
    "|                      | `feature_fraction` | Uses a subset of features for each tree | `0.6 - 1.0` |\n",
    "| **3. Regularization** | `lambda_l1` | L1 Regularization (Lasso) | `0 - 1` (Start with `0.1`) |\n",
    "|                      | `lambda_l2` | L2 Regularization (Ridge) | `0 - 1` |\n",
    "|                      | `min_gain_to_split` | Minimum gain required for a split | `0 - 0.1` |\n",
    "| **4. Speed Optimization** | `max_bin` | Number of bins for feature discretization | `255 - 512` |\n",
    "|                      | `bagging_fraction` | Uses only part of data per iteration | `0.6 - 0.9` |\n",
    "|                      | `bagging_freq` | Frequency of bagging (0 = off) | `1 - 5` |\n",
    "|                      | `num_threads` | Number of CPU cores used | `-1` (Auto-detect) |\n",
    "| **5. GPU Acceleration** | `device` | Enable GPU support | `\"gpu\"` |\n",
    "|                      | `gpu_platform_id` | Select GPU platform | `0` |\n",
    "|                      | `gpu_device_id` | Select GPU device | `0` |\n",
    "| **6. Tuning Strategies** | `Grid Search` | Exhaustive parameter search | `Slow but finds optimal values` |\n",
    "|                      | `Bayesian Optimization (Optuna)` | Finds best hyperparameters efficiently | `Fast & Recommended` |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Boosting Type** | **Description** | **Speed** | **Overfitting Prevention** | **Best For** |\n",
    "|------------------|---------------|----------|------------------------|--------------|\n",
    "| `gbdt` (Gradient Boosting Decision Trees) | Default method, sequentially improves predictions using gradient descent | ‚úÖ Fast | ‚ö†Ô∏è Can Overfit | General-purpose machine learning tasks |\n",
    "| `rf` (Random Forest Mode) | Trees are trained independently using bagging (random subsampling) | ‚ùå Slower | ‚úÖ Strong | High variance reduction (large datasets) |\n",
    "| `dart` (Dropouts Meet Multiple Additive Regression Trees) | Randomly drops trees to prevent overfitting, improving generalization | ‚ùå Slower | ‚úÖ Best | Small datasets prone to overfitting |\n",
    "| `goss` (Gradient-based One-Side Sampling) | Prioritizes high-gradient samples while reducing low-impact data points | üöÄ Fastest | ‚ö†Ô∏è Medium | Imbalanced datasets, faster training |\n",
    "\n",
    "### **üìå Choosing the Right Boosting Type**\n",
    "- ‚úÖ **Use `\"gbdt\"`** for most cases (default and well-balanced).  \n",
    "- ‚úÖ **Use `\"rf\"`** when you want a **random forest-like approach**.  \n",
    "- ‚úÖ **Use `\"dart\"`** for **small datasets prone to overfitting**.  \n",
    "- ‚úÖ **Use `\"goss\"`** for **imbalanced datasets with faster training**.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
